{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GauravLodhi_MT19063_A1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_MShl_-jXVt",
        "outputId": "9f2a0cfc-1719-483f-e014-130134feb4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Gaurav Lodhi \n",
        "MT19063\n",
        "Note: There may have some problems in running the file because of local libraries. This is my humble request either update the libraries or use the code on google colab.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# importing required packages\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Amino acid composition for features\n",
        "def get_features_AAC(df,list_peptides):\n",
        "  rows=df.size  #no of rows\n",
        "  df_f=pd.DataFrame(columns=list_peptides) #empty dataframe of list peptides\n",
        "  for index in range(rows) : \n",
        "    df_f.loc[index]=0\n",
        "    seq=df['Sequence'].loc[index]\n",
        "    for i in range(len(seq)):\n",
        "      col=seq[i]\n",
        "      # print(col)\n",
        "      df_f.loc[index][col]=df_f.loc[index][col]+1\n",
        "    df_f.loc[index]=df_f.loc[index]/(len(seq))\n",
        "  return  df_f\n",
        "\n",
        "\n",
        "#  Dipeptide composition for features\n",
        "def get_features_DP(df,d,list_peptides):\n",
        "  rows=df.size  #no of rows\n",
        "  df_f=pd.DataFrame(columns=list_peptides) #empty dataframe of list peptides\n",
        "  for index in range(rows) : \n",
        "    df_f.loc[index]=0\n",
        "    seq=df['Sequence'].loc[index]\n",
        "    for i in range(len(seq)-d-1):\n",
        "      col=seq[i]+seq[i+d+1]\n",
        "      # print(col)\n",
        "      df_f.loc[index][col]=df_f.loc[index][col]+1\n",
        "    df_f.loc[index]=df_f.loc[index]/(len(seq)-(d+1))\n",
        "  return  df_f\n",
        "\n",
        "\n",
        "# Extracting Features from sequences \n",
        "def Feature_Extraction_Storing():\n",
        "  print(\"Feature Extracting It may takes time if you want to avoid comment the method and use stored preprocessed data...\")\n",
        "  list_dfs_train=[]\n",
        "  list_dfs_test=[]\n",
        "  for d in range (6):\n",
        "    list_dfs_train.append(get_features_DP(data_train,d,list_dipeptides))\n",
        "    list_dfs_test.append(get_features_DP(data_test,d,list_dipeptides))\n",
        "    print(\"Complete for \",d,\"th order dipeptide\")\n",
        "  list_dfs_train.append(get_features_AAC(data_train,list_aminoAcids))\n",
        "  list_dfs_test.append(get_features_AAC(data_test,list_aminoAcids))\n",
        "  print(\"Feature Extraction Complete\")\n",
        "  train_data = pd.concat(list_dfs_train, axis=1, sort=False)\n",
        "  test_data = pd.concat(list_dfs_test, axis=1, sort=False)\n",
        "\n",
        "  # Storing the feature extracted dataset \n",
        "  train_data.to_pickle('train_ext_3.pkl')\n",
        "  test_data.to_pickle('test_ext_3.pkl')\n",
        "  print(\"Feature Extracting Complete and saved in a pickle file.\")\n",
        "\n",
        "\n",
        "# Cross validation for performance analysis.\n",
        "def CV(model,X,y):\n",
        "  cv_results = cross_val_score(model, X, y, cv=5)\n",
        "  print(cv_results)\n",
        "  print(np.mean(cv_results))\n",
        "\n",
        "\n",
        "def main(path_train,path_test,output_file_name):\n",
        "  # Reading the train.csv and valid.csv and seperating labels from the dataset\n",
        "  data=pd.read_csv(path_train)\n",
        "  data=data.rename(columns={' Sequence':'Sequence',' Type':'Type'})\n",
        "  print(Counter(data['Type']))\n",
        "  Y_train=data['Type']\n",
        "  Y_train=Y_train.replace({'NDNA':-1,'DNA':1})\n",
        "  data_train=data.drop(['ID','Type'],axis=1,inplace=False)\n",
        "\n",
        "  data_test=pd.read_csv(path_test)\n",
        "  data_test=data_test.rename(columns={' Sequence': 'Sequence'})\n",
        "  ids=data_test['ID']\n",
        "  data_test.drop(['ID'],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "  # making list of all the possible diapeptides and tripeptides.\n",
        "  list_tripeptides=[]\n",
        "  list_aminoAcids=['A','R','N','D','C','E','Q','G','H','I','L','K','M','F','P','S','T','W','Y','V','X']\n",
        "  list_dipeptides=[]\n",
        "  for x in list_aminoAcids:\n",
        "      for y in list_aminoAcids:\n",
        "          list_dipeptides.append(x+y)\n",
        "          for z in list_aminoAcids:\n",
        "              # print(x+y+z)\n",
        "              list_tripeptides.append(x+y+z)\n",
        "  print(len(list_tripeptides))\n",
        "  print(len(list_dipeptides))\n",
        "\n",
        "\n",
        "  # Feature Extraction it will take time to extract the features. To save some time comment it and use the prepared datastored in the pickle files.\n",
        "\n",
        "  #Feature_Extraction_Storing()\n",
        "\n",
        "\n",
        "  # Read prepared stored data\n",
        "  X_train=pd.read_pickle('train_ext_3.pkl',)\n",
        "  X_test=pd.read_pickle('test_ext_3.pkl')\n",
        "  #X_train = np.asarray(X_train).astype('float32')\n",
        "  #X_test=np.asarray(X_test).astype('float32')\n",
        "\n",
        "  # To check the information of the dataset\n",
        "  # train.info()\n",
        "  # test.info()\n",
        "\n",
        "  # SelectKBest for feature selection\n",
        "  print(\"Feature Selection is happening using SelectKBest.\")\n",
        "\n",
        "  select=SelectKBest(chi2, k=1900) \n",
        "  select.fit_transform(X_train, Y_train)\n",
        "  cols = select.get_support(indices=True)\n",
        "  X_train_selected = X_train.iloc[:,cols]\n",
        "  X_test_selected=X_test.iloc[:,cols]\n",
        "\n",
        "  # Using Oversampling to Balance the classes.\n",
        "\n",
        "  oversample = RandomOverSampler(sampling_strategy='minority',random_state=48)\n",
        "  # fit and apply the transform\n",
        "  X_over, y_over = oversample.fit_resample(X_train_selected, Y_train)\n",
        "  # summarize class distribution\n",
        "  print(Counter(y_over))\n",
        "\n",
        "\n",
        "  # find cross Validation of the model\n",
        "  print(\"Cross Validation for Model Preformance Analysis.\")\n",
        "  c=10\n",
        "  model=SVC(C= c, gamma= 1, kernel= 'rbf')\n",
        "  CV(model,X_over,y_over)\n",
        "\n",
        "  # GridSearchCV for Hyper parameter tunning.(commented because it takes time to process.)\n",
        "  # print(\"Grid Search for hyper parameter tunning.\")\n",
        "  # param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\n",
        "  # grid = GridSearchCV(SVC(),param_grid,refit = True, verbose=2,cv=5, scoring='accuracy')\n",
        "  # grid.fit(train,Y_train)\n",
        "  # grid.best_params_\n",
        "\n",
        "\n",
        "  # Model training\n",
        "  print(\"Model Trainig...\")\n",
        "  # model=SVC(C= 10, gamma= 1, kernel= 'rbf')\n",
        "  model.fit(X_over, y_over)\n",
        "\n",
        "  # prediction of the model\n",
        "  prediction=model.predict(X_test_selected)\n",
        "  print(Counter(prediction))\n",
        "\n",
        "  prediction=pd.DataFrame(data=prediction)\n",
        "  new_dataframe=pd.DataFrame(columns=['ID','Lable'])\n",
        "  new_dataframe['ID']=ids\n",
        "  new_dataframe['Lable']=prediction\n",
        "  new_dataframe=new_dataframe.set_index('ID')\n",
        "  # print(new_dataframe)\n",
        "\n",
        "  # Storing the result\n",
        "  new_dataframe.to_csv(output_file_name)\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# run code from here\n",
        "\n",
        "path_train='train.csv'\n",
        "path_test='valid.csv'\n",
        "output_file_name='output.csv'\n",
        "\n",
        "main(path_train,path_test,output_file_name)\n",
        "\n",
        "########################################################################\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e4389a96ae09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0moutput_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e4389a96ae09>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(path_train, path_test, output_file_name)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;31m# Reading the train.csv and valid.csv and seperating labels from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m' Sequence'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Sequence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' Type'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OiodiC_kGLw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}